{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "673a9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete\n",
    "import ta\n",
    "from tensortrade.env.default.actions import TensorTradeActionScheme\n",
    "\n",
    "from tensortrade.env.generic import ActionScheme, TradingEnv\n",
    "from tensortrade.core import Clock\n",
    "from tensortrade.oms.instruments import ExchangePair\n",
    "from tensortrade.oms.wallets import Portfolio\n",
    "from tensortrade.oms.orders import (\n",
    "    Order,\n",
    "    proportion_order,\n",
    "    TradeSide,\n",
    "    TradeType\n",
    ")\n",
    "\n",
    "\n",
    "class BSH(TensorTradeActionScheme):\n",
    "\n",
    "    registered_name = \"bsh\"\n",
    "\n",
    "    def __init__(self, cash: 'Wallet', asset: 'Wallet'):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.cash = cash\n",
    "        self.asset = asset\n",
    "\n",
    "        self.listeners = []\n",
    "        self.action = 0\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return Discrete(2)\n",
    "\n",
    "    def attach(self, listener):\n",
    "        self.listeners += [listener]\n",
    "        return self\n",
    "\n",
    "    def get_orders(self, action: int, portfolio: 'Portfolio'):\n",
    "        order = None\n",
    "\n",
    "        if abs(action - self.action) > 0:\n",
    "            src = self.cash if self.action == 0 else self.asset\n",
    "            tgt = self.asset if self.action == 0 else self.cash\n",
    "            order = proportion_order(portfolio, src, tgt, 1.0)\n",
    "            self.action = action\n",
    "\n",
    "        for listener in self.listeners:\n",
    "            listener.on_action(action)\n",
    "\n",
    "        return [order]\n",
    "\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.action = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35292a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom classes for transformer predictor:\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "#Timevector layer\n",
    "class Time2Vector(Layer):\n",
    "  def __init__(self, seq_len, **kwargs):\n",
    "    super(Time2Vector, self).__init__()\n",
    "    self.seq_len = seq_len\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    '''Initialize weights and biases with shape (batch, seq_len)'''\n",
    "    self.weights_linear = self.add_weight(name='weight_linear',\n",
    "                                shape=(int(self.seq_len),),\n",
    "                                initializer='uniform',\n",
    "                                trainable=True)\n",
    "    \n",
    "    self.bias_linear = self.add_weight(name='bias_linear',\n",
    "                                shape=(int(self.seq_len),),\n",
    "                                initializer='uniform',\n",
    "                                trainable=True)\n",
    "    \n",
    "    self.weights_periodic = self.add_weight(name='weight_periodic',\n",
    "                                shape=(int(self.seq_len),),\n",
    "                                initializer='uniform',\n",
    "                                trainable=True)\n",
    "\n",
    "    self.bias_periodic = self.add_weight(name='bias_periodic',\n",
    "                                shape=(int(self.seq_len),),\n",
    "                                initializer='uniform',\n",
    "                                trainable=True)\n",
    "\n",
    "  def call(self, x):\n",
    "    '''Calculate linear and periodic time features'''\n",
    "    x = tf.math.reduce_mean(x[:,:,:4], axis=-1) \n",
    "    time_linear = self.weights_linear * x + self.bias_linear # Linear time feature\n",
    "    time_linear = tf.expand_dims(time_linear, axis=-1) # Add dimension (batch, seq_len, 1)\n",
    "    \n",
    "    time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)\n",
    "    time_periodic = tf.expand_dims(time_periodic, axis=-1) # Add dimension (batch, seq_len, 1)\n",
    "    return tf.concat([time_linear, time_periodic], axis=-1) # shape = (batch, seq_len, 2)\n",
    "   \n",
    "  def get_config(self): # Needed for saving and loading model with custom layer\n",
    "    config = super().get_config().copy()\n",
    "    config.update({'seq_len': self.seq_len})\n",
    "    return config\n",
    "\n",
    "#Transformer Layers\n",
    "class SingleAttention(Layer):\n",
    "  def __init__(self, d_k, d_v):\n",
    "    super(SingleAttention, self).__init__()\n",
    "    self.d_k = d_k\n",
    "    self.d_v = d_v\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.query = Dense(self.d_k, \n",
    "                       input_shape=input_shape, \n",
    "                       kernel_initializer='glorot_uniform', \n",
    "                       bias_initializer='glorot_uniform')\n",
    "    \n",
    "    self.key = Dense(self.d_k, \n",
    "                     input_shape=input_shape, \n",
    "                     kernel_initializer='glorot_uniform', \n",
    "                     bias_initializer='glorot_uniform')\n",
    "    \n",
    "    self.value = Dense(self.d_v, \n",
    "                       input_shape=input_shape, \n",
    "                       kernel_initializer='glorot_uniform', \n",
    "                       bias_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
    "    q = self.query(inputs[0])\n",
    "    k = self.key(inputs[1])\n",
    "\n",
    "    attn_weights = tf.matmul(q, k, transpose_b=True)\n",
    "    attn_weights = tf.map_fn(lambda x: x/np.sqrt(self.d_k), attn_weights)\n",
    "    attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
    "    \n",
    "    v = self.value(inputs[2])\n",
    "    attn_out = tf.matmul(attn_weights, v)\n",
    "    return attn_out    \n",
    "\n",
    "#############################################################################\n",
    "\n",
    "class MultiAttention(Layer):\n",
    "  def __init__(self, d_k, d_v, n_heads):\n",
    "    super(MultiAttention, self).__init__()\n",
    "    self.d_k = d_k\n",
    "    self.d_v = d_v\n",
    "    self.n_heads = n_heads\n",
    "    self.attn_heads = list()\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    for n in range(self.n_heads):\n",
    "      self.attn_heads.append(SingleAttention(self.d_k, self.d_v))  \n",
    "    \n",
    "    # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1]=7 \n",
    "    self.linear = Dense(input_shape[0][-1], \n",
    "                        input_shape=input_shape, \n",
    "                        kernel_initializer='glorot_uniform', \n",
    "                        bias_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    attn = [self.attn_heads[i](inputs) for i in range(self.n_heads)]\n",
    "    concat_attn = tf.concat(attn, axis=-1)\n",
    "    multi_linear = self.linear(concat_attn)\n",
    "    return multi_linear   \n",
    "\n",
    "#############################################################################\n",
    "\n",
    "class TransformerEncoder(Layer):\n",
    "  def __init__(self, d_k, d_v, n_heads, ff_dim, dropout=0.1, **kwargs):\n",
    "    super(TransformerEncoder, self).__init__()\n",
    "    self.d_k = d_k\n",
    "    self.d_v = d_v\n",
    "    self.n_heads = n_heads\n",
    "    self.ff_dim = ff_dim\n",
    "    self.attn_heads = list()\n",
    "    self.dropout_rate = dropout\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads)\n",
    "    self.attn_dropout = Dropout(self.dropout_rate)\n",
    "    self.attn_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
    "\n",
    "    self.ff_conv1D_1 = Conv1D(filters=self.ff_dim, kernel_size=1, activation='relu')\n",
    "    # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1] = 7 \n",
    "    self.ff_conv1D_2 = Conv1D(filters=input_shape[0][-1], kernel_size=1) \n",
    "    self.ff_dropout = Dropout(self.dropout_rate)\n",
    "    self.ff_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)    \n",
    "  \n",
    "  def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
    "    attn_layer = self.attn_multi(inputs)\n",
    "    attn_layer = self.attn_dropout(attn_layer)\n",
    "    attn_layer = self.attn_normalize(inputs[0] + attn_layer)\n",
    "\n",
    "    ff_layer = self.ff_conv1D_1(attn_layer)\n",
    "    ff_layer = self.ff_conv1D_2(ff_layer)\n",
    "    ff_layer = self.ff_dropout(ff_layer)\n",
    "    ff_layer = self.ff_normalize(inputs[0] + ff_layer)\n",
    "    return ff_layer \n",
    "\n",
    "  def get_config(self): # Needed for saving and loading model with custom layer\n",
    "    config = super().get_config().copy()\n",
    "    config.update({'d_k': self.d_k,\n",
    "                   'd_v': self.d_v,\n",
    "                   'n_heads': self.n_heads,\n",
    "                   'ff_dim': self.ff_dim,\n",
    "                   'attn_heads': self.attn_heads,\n",
    "                   'dropout_rate': self.dropout_rate})\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd11151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensortrade.env.default.rewards import TensorTradeRewardScheme\n",
    "from tensortrade.feed.core import Stream, DataFeed\n",
    "from tensortrade.oms.instruments import Instrument\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "USD = Instrument(\"USD\", 2, \"U.S. Dollar\")\n",
    "TTC = Instrument(\"TTC\", 8, \"TensorTrade Coin\")\n",
    "\n",
    "class MXRewardScheme(TensorTradeRewardScheme):\n",
    "    \"\"\"A reward scheme that rewards the agent for increasing its net worth,\n",
    "        while penalizing more volatile strategies.\n",
    "        Parameters\n",
    "        ----------\n",
    "        :param: return_algorithm : {'sharpe', 'sortino'}, Default 'sharpe'.\n",
    "            The risk-adjusted return metric to use.\n",
    "        :param: risk_free_rate : float, Default 0.\n",
    "            The risk free rate of returns to use for calculating metrics.\n",
    "        :param: target_returns : float, Default 0\n",
    "            The target returns per period for use in calculating the sortino ratio.\n",
    "        :param: window_size : int\n",
    "            The size of the look back window for computing the reward.\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 return_algorithm: str = 'sharpe',\n",
    "                 risk_free_rate: float = 0.,\n",
    "                 target_returns: float = 0.,\n",
    "                 window_size: int = 1) -> None:\n",
    "        algorithm = self.default('return_algorithm', return_algorithm)\n",
    "\n",
    "        assert algorithm in ['sharpe', 'diff_sharpe', 'sortino']\n",
    "\n",
    "        if algorithm == 'sharpe':\n",
    "            return_algorithm = self._sharpe_ratio\n",
    "        elif algorithm == 'sortino':\n",
    "            return_algorithm = self._sortino_ratio\n",
    "        elif algorithm == 'diff_sharpe':\n",
    "            return_algorithm = self._diff_sharpe_ratio\n",
    "\n",
    "        self._return_algorithm = return_algorithm\n",
    "        self._risk_free_rate = self.default('risk_free_rate', risk_free_rate)\n",
    "        self._target_returns = self.default('target_returns', target_returns)\n",
    "        self._window_size = self.default('window_size', window_size)\n",
    "\n",
    "    def _sharpe_ratio(self, returns: 'pd.Series') -> float:\n",
    "        \"\"\"Computes the sharpe ratio for a given series of a returns.\n",
    "        Parameters\n",
    "        ----------\n",
    "        returns : `pd.Series`\n",
    "            The returns for the `portfolio`.\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The sharpe ratio for the given series of a `returns`.\n",
    "        References\n",
    "        ----------\n",
    "        .. [1] https://en.wikipedia.org/wiki/Sharpe_ratio\n",
    "        \"\"\"\n",
    "        return (np.mean(returns) - self._risk_free_rate + 1e-9) / (np.std(returns) + 1e-9)\n",
    "\n",
    "    def _diff_sharpe_ratio(self, returns: 'pd.Series') -> float:\n",
    "        \"\"\"Computes the differential sharpe ratio over a given series of returns\n",
    "        Parameters\n",
    "        ----------\n",
    "        returns : `pd.Series`\n",
    "            The returns for the 'portfolio'\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The differential sharpe ratio for the given series of a `returns`\n",
    "        References\n",
    "        ----------\n",
    "        .. [1] https://proceedings.neurips.cc/paper/1998/file/4e6cd95227cb0c280e99a195be5f6615-Paper.pdf\n",
    "        .. [2] https://github.com/AchillesJJ/DSR\n",
    "        \"\"\"\n",
    "        np.seterr('raise')\n",
    "        eta = 0.004\n",
    "\n",
    "        A = np.mean(returns[-1:])\n",
    "        B = np.mean(returns[-1:]**2)\n",
    "        delta_A = np.mean(returns) - A\n",
    "        delta_B = np.mean(returns)**2 - B\n",
    "        upper = ((B * delta_A - 0.5*A*delta_B) + 1e-9)\n",
    "        lower = (B-A**2 + 1e-9)**(3/2)\n",
    "\n",
    "        if lower == (0 or np.isnan(lower)):\n",
    "            print(f\"A:{A}\\n\"\n",
    "                  f\"B:{B}\\n\"\n",
    "                  f\"delta_A:{delta_A}\\n\"\n",
    "                  f\"delta_B:{delta_B}\\n\"\n",
    "                  f\"upper:{upper}\\n\"\n",
    "                  f\"lower:{lower}\\n\")\n",
    "                  # f\"reward:{dt*eta}\\n\")\n",
    "        dt = upper / lower\n",
    "\n",
    "        return dt * eta\n",
    "\n",
    "    def _sortino_ratio(self, returns: 'pd.Series') -> float:\n",
    "        \"\"\"Computes the sortino ratio for a given series of a returns.\n",
    "        Parameters\n",
    "        ----------\n",
    "        returns : `pd.Series`\n",
    "            The returns for the `portfolio`.\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The sortino ratio for the given series of a `returns`.\n",
    "        References\n",
    "        ----------\n",
    "        .. [1] https://en.wikipedia.org/wiki/Sortino_ratio\n",
    "        \"\"\"\n",
    "        downside_returns = returns.copy()\n",
    "        downside_returns[returns < self._target_returns] = returns ** 2\n",
    "\n",
    "        expected_return = np.mean(returns)\n",
    "        downside_std = np.sqrt(np.std(downside_returns))\n",
    "\n",
    "        result = (expected_return - self._risk_free_rate + 1e-9) / (downside_std + 1e-9)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_reward(self, portfolio: 'Portfolio') -> float:\n",
    "        \"\"\"Computes the reward corresponding to the selected risk-adjusted return metric.\n",
    "        Parameters\n",
    "        ----------\n",
    "        portfolio : `Portfolio`\n",
    "            The current portfolio being used by the environment.\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The reward corresponding to the selected risk-adjusted return metric.\n",
    "        \"\"\"\n",
    "        net_worths = [nw['net_worth'] for nw in portfolio.performance.values()][-(self._window_size + 1):]\n",
    "        returns = pd.Series(net_worths).pct_change().dropna()\n",
    "        risk_adjusted_return = self._return_algorithm(returns)\n",
    "        return risk_adjusted_return\n",
    "\n",
    "class PBR(TensorTradeRewardScheme):\n",
    "\n",
    "    registered_name = \"pbr\"\n",
    "\n",
    "    def __init__(self, price: 'Stream'):\n",
    "        super().__init__()\n",
    "        self.position = -1\n",
    "\n",
    "        r = Stream.sensor(price, lambda p: p.value, dtype=\"float\").diff()\n",
    "        position = Stream.sensor(self, lambda rs: rs.position, dtype=\"float\")\n",
    "\n",
    "        reward = (r * position).fillna(0).rename(\"reward\")\n",
    "\n",
    "        self.feed = DataFeed([reward])\n",
    "        self.feed.compile()\n",
    "\n",
    "    def on_action(self, action: int):\n",
    "        self.position = -1 if action == 0 else 1\n",
    "\n",
    "    def get_reward(self, portfolio: 'Portfolio'):\n",
    "        return self.feed.next()[\"reward\"]\n",
    "\n",
    "    def reset(self):\n",
    "        self.position = -1\n",
    "        self.feed.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d305116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensortrade.env.generic import Renderer\n",
    "\n",
    "\n",
    "class PositionChangeChart(Renderer):\n",
    "\n",
    "    def __init__(self, color: str = \"orange\"):\n",
    "        self.color = \"orange\"\n",
    "\n",
    "    def render(self, env, **kwargs):\n",
    "        history = pd.DataFrame(env.observer.renderer_history)\n",
    "\n",
    "        actions = list(history.action)\n",
    "        p = list(history.price)\n",
    "\n",
    "        buy = {}\n",
    "        sell = {}\n",
    "\n",
    "        for i in range(len(actions) - 1):\n",
    "            a1 = actions[i]\n",
    "            a2 = actions[i + 1]\n",
    "\n",
    "            if a1 != a2:\n",
    "                if a1 == 0 and a2 == 1:\n",
    "                    buy[i] = p[i]\n",
    "                else:\n",
    "                    sell[i] = p[i]\n",
    "\n",
    "        buy = pd.Series(buy)\n",
    "        sell = pd.Series(sell)\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        fig.suptitle(\"Performance\")\n",
    "\n",
    "        axs[0].plot(np.arange(len(p)), p, label=\"price\", color=self.color)\n",
    "        axs[0].scatter(buy.index, buy.values, marker=\"^\", color=\"green\")\n",
    "        axs[0].scatter(sell.index, sell.values, marker=\"^\", color=\"red\")\n",
    "        axs[0].set_title(\"Trading Chart\")\n",
    "        \n",
    "        performance = pd.DataFrame.from_dict(env.action_scheme.portfolio.performance, orient='index')\n",
    "        performance.plot(ax=axs[1])\n",
    "        axs[1].set_title(\"Net Worth\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18e9a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframe = pd.read_csv('BTC-USD_Cleaned_Megafile_01_01_2020-12_31_2020.csv')\n",
    "dataframe.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d391321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "import tensortrade.env.default as default\n",
    "\n",
    "from tensortrade.feed.core import DataFeed, Stream\n",
    "from tensortrade.oms.exchanges import Exchange\n",
    "from tensortrade.oms.services.execution.simulated import execute_order\n",
    "from tensortrade.oms.wallets import Wallet, Portfolio\n",
    "\n",
    "\n",
    "\n",
    "def create_env(config):\n",
    "    data=config[\"data\"]\n",
    "    c = Stream.source(list(data[\"close\"]), dtype=\"float\").rename(\"USD-TTC\")\n",
    "    #sharpe=Stream.source(list(c_sharpe_ratio), dtype=\"float\").rename(\"sharpe_ratio\")\n",
    "    #vola=Stream.source(list(c_volatility), dtype=\"float\").rename(\"volatility\")\n",
    "    \n",
    "    o = Stream.source(list(data[\"open\"]), dtype=\"float\").rename(\"open\")\n",
    "    h = Stream.source(list(data[\"high\"]), dtype=\"float\").rename(\"high\")\n",
    "    l = Stream.source(list(data[\"low\"]), dtype=\"float\").rename(\"low\")\n",
    "    v = Stream.source(list(data[\"volume\"]), dtype=\"float\").rename(\"volume\")\n",
    "    bitfinex = Exchange(\"bitfinex\", service=execute_order)(\n",
    "        c\n",
    "    )\n",
    "\n",
    "    cash = Wallet(bitfinex, config[\"USD\"] * USD)\n",
    "    asset = Wallet(bitfinex, config[\"BTC\"] * TTC)\n",
    "\n",
    "    portfolio = Portfolio(USD, [\n",
    "        cash,\n",
    "        asset\n",
    "    ])\n",
    "\n",
    "    feed = DataFeed([\n",
    "    o,\n",
    "    #o.rolling(window=10).mean().rename(\"o_fast\"),\n",
    "    #o.rolling(window=50).mean().rename(\"o_medium\"),\n",
    "    #o.rolling(window=100).mean().rename(\"o_slow\"),\n",
    "    o.log().diff().fillna(0).rename(\"o_lr\"),\n",
    "        \n",
    "    h,\n",
    "    #h.rolling(window=10).mean().rename(\"h_fast\"),\n",
    "    #h.rolling(window=50).mean().rename(\"h_medium\"),\n",
    "    #h.rolling(window=100).mean().rename(\"h_slow\"),\n",
    "    h.log().diff().fillna(0).rename(\"h_lr\"),\n",
    "    \n",
    "        \n",
    "    l,\n",
    "    #l.rolling(window=10).mean().rename(\"l_fast\"),\n",
    "    #l.rolling(window=50).mean().rename(\"l_medium\"),\n",
    "    #l.rolling(window=100).mean().rename(\"l_slow\"),\n",
    "    l.log().diff().fillna(0).rename(\"l_lr\"),\n",
    "        \n",
    "    c,\n",
    "    #c.rolling(window=10).mean().rename(\"c_fast\"),\n",
    "    #c.rolling(window=50).mean().rename(\"c_medium\"),\n",
    "    #c.rolling(window=100).mean().rename(\"c_slow\"),\n",
    "    c.log().diff().fillna(0).rename(\"c_lr\"), \n",
    "    \n",
    "    v, \n",
    "    Stream.source(list(data[\"rel_low\"]), dtype=\"float\").rename(\"rel_low\"),\n",
    "    Stream.source(list(data[\"rel_high\"]), dtype=\"float\").rename(\"rel_high\"),\n",
    "    Stream.source(list(data[\"rel_open\"]), dtype=\"float\").rename(\"rel_open\"),\n",
    "    Stream.source(list(data[\"rel_close\"]), dtype=\"float\").rename(\"rel_close\"),\n",
    "    ])\n",
    "\n",
    "    #reward_scheme = MXRewardScheme(return_algorithm='diff_sharpe')\n",
    "    reward_scheme = PBR(price=o)\n",
    "\n",
    "    action_scheme = BSH(\n",
    "        cash=cash,\n",
    "        asset=asset\n",
    "    ).attach(reward_scheme) #remove the .attach for non PBR reward schemes\n",
    "\n",
    "    renderer_feed = DataFeed([\n",
    "        Stream.source(list(data[\"close\"]), dtype=\"float\").rename(\"price\"),\n",
    "        Stream.sensor(action_scheme, lambda s: s.action, dtype=\"float\").rename(\"action\")\n",
    "    ])\n",
    "\n",
    "    environment = default.create(\n",
    "        feed=feed,\n",
    "        portfolio=portfolio,\n",
    "        action_scheme=action_scheme,\n",
    "        reward_scheme=reward_scheme,\n",
    "        renderer_feed=renderer_feed,\n",
    "        renderer=PositionChangeChart(),\n",
    "        window_size=config[\"window_size\"],\n",
    "        max_allowed_loss=0.6\n",
    "    )\n",
    "    return environment\n",
    "\n",
    "register_env(\"TradingEnv\", create_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a653eee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-07 17:56:23,289\tINFO services.py:1267 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2021-06-07 17:56:39,109\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/1 GPUs, 0.0/3.48 GiB heap, 0.0/1.74 GiB objects<br>Result logdir: C:\\Users\\Annoy\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TradingEnv_3c444_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TradingEnv_3c444_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-07_17-57-03\n",
      "  done: false\n",
      "  episode_len_mean: 344.09090909090907\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 623.5999999999931\n",
      "  episode_reward_mean: -13.915454545457413\n",
      "  episode_reward_min: -348.4099999999971\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 11\n",
      "  experiment_id: 6da27ddd05464703be53231eb69849db\n",
      "  hostname: MSI\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 8.0e-07\n",
      "          entropy: 0.6931188702583313\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 2.2926852750515536e-05\n",
      "          policy_loss: -0.008015550498384982\n",
      "          total_loss: 0.4642219031229615\n",
      "          vf_explained_var: 0.0009087827056646347\n",
      "          vf_loss: 0.9583281148225069\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.254.29\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.9\n",
      "    gpu_util_percent0: 0.0\n",
      "    ram_util_percent: 70.14285714285715\n",
      "    vram_util_percent0: 0.0135498046875\n",
      "  pid: 9744\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04323742682264615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.5249006990729734\n",
      "    mean_inference_ms: 0.9276616635902976\n",
      "    mean_raw_obs_processing_ms: 0.12849295505789687\n",
      "  time_since_restore: 15.318211317062378\n",
      "  time_this_iter_s: 15.318211317062378\n",
      "  time_total_s: 15.318211317062378\n",
      "  timers:\n",
      "    learn_throughput: 460.65\n",
      "    learn_time_ms: 8683.388\n",
      "    sample_throughput: 604.045\n",
      "    sample_time_ms: 6622.025\n",
      "    update_time_ms: 0.0\n",
      "  timestamp: 1623103023\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 3c444_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/1 GPUs, 0.0/3.48 GiB heap, 0.0/1.74 GiB objects<br>Result logdir: C:\\Users\\Annoy\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TradingEnv_3c444_00000</td><td>RUNNING </td><td>192.168.254.29:9744</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         15.3182</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-13.9155</td><td style=\"text-align: right;\">               623.6</td><td style=\"text-align: right;\">             -348.41</td><td style=\"text-align: right;\">           344.091</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TradingEnv_3c444_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-07_17-57-18\n",
      "  done: false\n",
      "  episode_len_mean: 338.2608695652174\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 623.5999999999931\n",
      "  episode_reward_mean: -5.763478260871362\n",
      "  episode_reward_min: -469.7300000000023\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 23\n",
      "  experiment_id: 6da27ddd05464703be53231eb69849db\n",
      "  hostname: MSI\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0007000000000000001\n",
      "          entropy: 0.6361785437911749\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.05854135821573436\n",
      "          policy_loss: -0.2962749097496271\n",
      "          total_loss: 0.16035953437676653\n",
      "          vf_explained_var: 0.04728589206933975\n",
      "          vf_loss: 0.9142841715365648\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.254.29\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.392307692307693\n",
      "    gpu_util_percent0: 0.0\n",
      "    ram_util_percent: 71.23076923076923\n",
      "    vram_util_percent0: 0.0135498046875\n",
      "  pid: 9744\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04294422962538683\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.5238501133912061\n",
      "    mean_inference_ms: 0.8967650923135422\n",
      "    mean_raw_obs_processing_ms: 0.13065351826098046\n",
      "  time_since_restore: 30.6442654132843\n",
      "  time_this_iter_s: 15.326054096221924\n",
      "  time_total_s: 30.6442654132843\n",
      "  timers:\n",
      "    learn_throughput: 448.165\n",
      "    learn_time_ms: 8925.289\n",
      "    sample_throughput: 626.437\n",
      "    sample_time_ms: 6385.319\n",
      "    update_time_ms: 1.014\n",
      "  timestamp: 1623103038\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 3c444_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/1 GPUs, 0.0/3.48 GiB heap, 0.0/1.74 GiB objects<br>Result logdir: C:\\Users\\Annoy\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TradingEnv_3c444_00000</td><td>RUNNING </td><td>192.168.254.29:9744</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         30.6443</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-5.76348</td><td style=\"text-align: right;\">               623.6</td><td style=\"text-align: right;\">             -469.73</td><td style=\"text-align: right;\">           338.261</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TradingEnv_3c444_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-07_17-57-34\n",
      "  done: false\n",
      "  episode_len_mean: 336.57142857142856\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1222.8499999999985\n",
      "  episode_reward_mean: 164.9354285714272\n",
      "  episode_reward_min: -469.7300000000023\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 35\n",
      "  experiment_id: 6da27ddd05464703be53231eb69849db\n",
      "  hostname: MSI\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.15000000000000002\n",
      "          cur_lr: 0.00030000000000000003\n",
      "          entropy: 0.40287499595433474\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.1262293483596295\n",
      "          policy_loss: -0.26622440543724224\n",
      "          total_loss: 0.15465577799477614\n",
      "          vf_explained_var: 0.06405028700828552\n",
      "          vf_loss: 0.8119490593671799\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.254.29\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.55\n",
      "    gpu_util_percent0: 0.0\n",
      "    ram_util_percent: 70.775\n",
      "    vram_util_percent0: 0.0135498046875\n",
      "  pid: 9744\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03973965389958427\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.5275377007867738\n",
      "    mean_inference_ms: 0.882279657750693\n",
      "    mean_raw_obs_processing_ms: 0.13008796232755213\n",
      "  time_since_restore: 45.628076791763306\n",
      "  time_this_iter_s: 14.983811378479004\n",
      "  time_total_s: 45.628076791763306\n",
      "  timers:\n",
      "    learn_throughput: 450.948\n",
      "    learn_time_ms: 8870.203\n",
      "    sample_throughput: 631.89\n",
      "    sample_time_ms: 6330.22\n",
      "    update_time_ms: 1.009\n",
      "  timestamp: 1623103054\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 3c444_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/1 GPUs, 0.0/3.48 GiB heap, 0.0/1.74 GiB objects<br>Result logdir: C:\\Users\\Annoy\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TradingEnv_3c444_00000</td><td>RUNNING </td><td>192.168.254.29:9744</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         45.6281</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> 164.935</td><td style=\"text-align: right;\">             1222.85</td><td style=\"text-align: right;\">             -469.73</td><td style=\"text-align: right;\">           336.571</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TradingEnv_3c444_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-07_17-57-49\n",
      "  done: false\n",
      "  episode_len_mean: 337.1063829787234\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2044.639999999994\n",
      "  episode_reward_mean: 469.6014893617002\n",
      "  episode_reward_min: -469.7300000000023\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 47\n",
      "  experiment_id: 6da27ddd05464703be53231eb69849db\n",
      "  hostname: MSI\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.22500000000000003\n",
      "          cur_lr: 9.800000000000001e-05\n",
      "          entropy: 0.23104535974562168\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.04587947437539697\n",
      "          policy_loss: -0.11940986511763185\n",
      "          total_loss: 0.10209224373102188\n",
      "          vf_explained_var: 0.08195527642965317\n",
      "          vf_loss: 0.42697936575859785\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.254.29\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.885714285714286\n",
      "    gpu_util_percent0: 0.0\n",
      "    ram_util_percent: 70.39999999999999\n",
      "    vram_util_percent0: 0.0135498046875\n",
      "  pid: 9744\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037822356824002056\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.5279692908144497\n",
      "    mean_inference_ms: 0.8788739707400901\n",
      "    mean_raw_obs_processing_ms: 0.12781634555318636\n",
      "  time_since_restore: 60.96480107307434\n",
      "  time_this_iter_s: 15.336724281311035\n",
      "  time_total_s: 60.96480107307434\n",
      "  timers:\n",
      "    learn_throughput: 449.899\n",
      "    learn_time_ms: 8890.881\n",
      "    sample_throughput: 630.712\n",
      "    sample_time_ms: 6342.034\n",
      "    update_time_ms: 1.247\n",
      "  timestamp: 1623103069\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 3c444_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/1 GPUs, 0.0/3.48 GiB heap, 0.0/1.74 GiB objects<br>Result logdir: C:\\Users\\Annoy\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TradingEnv_3c444_00000</td><td>RUNNING </td><td>192.168.254.29:9744</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         60.9648</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\"> 469.601</td><td style=\"text-align: right;\">             2044.64</td><td style=\"text-align: right;\">             -469.73</td><td style=\"text-align: right;\">           337.106</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TradingEnv_3c444_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-06-07_17-58-05\n",
      "  done: true\n",
      "  episode_len_mean: 333.91525423728814\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2047.1699999999983\n",
      "  episode_reward_mean: 681.4845762711839\n",
      "  episode_reward_min: -469.7300000000023\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 59\n",
      "  experiment_id: 6da27ddd05464703be53231eb69849db\n",
      "  hostname: MSI\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.3375\n",
      "          cur_lr: 9.400000000000001e-05\n",
      "          entropy: 0.1639405540190637\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.00860477308742702\n",
      "          policy_loss: -0.07632988359546289\n",
      "          total_loss: 0.025280765490606427\n",
      "          vf_explained_var: 0.13750013709068298\n",
      "          vf_loss: 0.2006918927654624\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.254.29\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.953846153846156\n",
      "    gpu_util_percent0: 0.0\n",
      "    ram_util_percent: 70.37692307692306\n",
      "    vram_util_percent0: 0.0135498046875\n",
      "  pid: 9744\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0372048264681891\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.527498087818551\n",
      "    mean_inference_ms: 0.878446525015033\n",
      "    mean_raw_obs_processing_ms: 0.12594534394136422\n",
      "  time_since_restore: 76.08181643486023\n",
      "  time_this_iter_s: 15.117015361785889\n",
      "  time_total_s: 76.08181643486023\n",
      "  timers:\n",
      "    learn_throughput: 451.312\n",
      "    learn_time_ms: 8863.058\n",
      "    sample_throughput: 630.222\n",
      "    sample_time_ms: 6346.969\n",
      "    update_time_ms: 1.397\n",
      "  timestamp: 1623103085\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 3c444_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/12 CPUs, 0/1 GPUs, 0.0/3.48 GiB heap, 0.0/1.74 GiB objects<br>Result logdir: C:\\Users\\Annoy\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TradingEnv_3c444_00000</td><td>RUNNING </td><td>192.168.254.29:9744</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         76.0818</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> 681.485</td><td style=\"text-align: right;\">             2047.17</td><td style=\"text-align: right;\">             -469.73</td><td style=\"text-align: right;\">           333.915</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/1 GPUs, 0.0/3.48 GiB heap, 0.0/1.74 GiB objects<br>Result logdir: C:\\Users\\Annoy\\ray_results\\PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TradingEnv_3c444_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         76.0818</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> 681.485</td><td style=\"text-align: right;\">             2047.17</td><td style=\"text-align: right;\">             -469.73</td><td style=\"text-align: right;\">           333.915</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-07 17:58:07,258\tINFO tune.py:549 -- Total run time: 90.88 seconds (87.05 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "ray.init(log_to_driver=False,ignore_reinit_error=True)\n",
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\n",
    "      \"episode_reward_mean\": 500\n",
    "    },\n",
    "    config={\n",
    "        \"env\": \"TradingEnv\",\n",
    "        \"env_config\": {\n",
    "            \"window_size\": 25,\n",
    "            \"data\":dataframe,\n",
    "            \"USD\":100,\n",
    "            \"BTC\":0\n",
    "        },\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"framework\": \"torch\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-7,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    },\n",
    "    checkpoint_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "789654f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c49d0977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-07 18:00:07,172\tDEBUG rollout_worker.py:1122 -- Creating policy for default_policy\n",
      "2021-06-07 18:00:07,183\tDEBUG catalog.py:631 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x000001D38DE73C10>: Box(-inf, inf, (25, 13), float32) -> (25, 13)\n",
      "2021-06-07 18:00:07,184\tINFO torch_policy.py:112 -- TorchPolicy running on CPU.\n",
      "2021-06-07 18:00:07,216\tINFO rollout_worker.py:1161 -- Built policy map: {'default_policy': <ray.rllib.policy.policy_template.PPOTorchPolicy object at 0x000001D38DE73190>}\n",
      "2021-06-07 18:00:07,217\tINFO rollout_worker.py:1162 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x000001D38DE73C10>}\n",
      "2021-06-07 18:00:07,217\tDEBUG rollout_worker.py:531 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "2021-06-07 18:00:07,218\tINFO rollout_worker.py:563 -- Built filter map: {'default_policy': MeanStdFilter((25, 13), True, True, None, (n=0, mean_mean=0.0, mean_std=0.0), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
      "2021-06-07 18:00:07,219\tDEBUG rollout_worker.py:678 -- Created rollout worker with env None (None), policies {'default_policy': <ray.rllib.policy.policy_template.PPOTorchPolicy object at 0x000001D38DE73190>}\n",
      "2021-06-07 18:00:07,264\tINFO trainable.py:377 -- Restored on 192.168.254.29 from checkpoint: C:\\Users\\Annoy\\ray_results\\PPO\\PPO_TradingEnv_3c444_00000_0_2021-06-07_17-56-39\\checkpoint_000005\\checkpoint-5\n",
      "2021-06-07 18:00:07,266\tINFO trainable.py:385 -- Current state after restoring: {'_iteration': 5, '_timesteps_total': None, '_time_total': 76.08181643486023, '_episodes_total': 59}\n"
     ]
    }
   ],
   "source": [
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "# Get checkpoint\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\",mode=\"max\"),\n",
    "    metric=\"episode_reward_mean\"\n",
    ")\n",
    "checkpoint_path = checkpoints[0][0]\n",
    "\n",
    "# Restore agent\n",
    "agent = ppo.PPOTrainer(\n",
    "    env=\"TradingEnv\",\n",
    "    config={\n",
    "        \"env_config\": {\n",
    "            \"window_size\": 25,\n",
    "            \"data\":dataframe,\n",
    "            \"USD\":100,\n",
    "            \"BTC\":0\n",
    "        },\n",
    "        \"framework\": \"torch\",\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-7,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    }\n",
    ")\n",
    "agent.restore(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a239631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "env = create_env({\n",
    "    \"window_size\": 25,\n",
    "    \"data\":dataframe,\n",
    "    \"USD\":100,\n",
    "    \"BTC\":0\n",
    "})\n",
    "\n",
    "# Run until episode ends\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.compute_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
